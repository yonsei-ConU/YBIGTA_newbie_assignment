{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53cd899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (0.30.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from groq) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (0.34.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shon\\anaconda3\\envs\\prompting\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq python-dotenv numpy tqdm datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "249253df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(0)\n",
    "\n",
    "client = Groq()\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "gsm8k_train = gsm8k_dataset[\"train\"]\n",
    "gsm8k_test  = gsm8k_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d75bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_using_Llama(\n",
    "        prompt: str,\n",
    "        model: str = \"llama3-8b-8192\"\n",
    "    ):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant that solves math problems.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=model,\n",
    "            temperature=0.3, ### 수정해도 됩니다!\n",
    "            stream=False\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"API call error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef65ad3",
   "metadata": {},
   "source": [
    "#### 응답 잘 나오는지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a37fad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! It's great to meet you! I'm here to help you with any problems or questions you might have. What's on your mind? Do you have a specific issue you'd like to tackle, or are you just looking for some general assistance? Let me know, and I'll do my best to guide you through it step by step!\n"
     ]
    }
   ],
   "source": [
    "response = generate_response_using_Llama(\n",
    "    prompt=\"Hello world!\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988803ae",
   "metadata": {},
   "source": [
    "#### GSM8K 데이터셋 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbd177eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Question]\n",
      "Janet’s ducks lay 16 eggs per day\n",
      " She eats three for breakfast every morning and bakes muffins for her friends every day with four\n",
      " She sells the remainder at the farmers' market daily for $2 per fresh duck egg\n",
      " How much in dollars does she make every day at the farmers' market?\n",
      "====================================================================================================\n",
      "[Answer]\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n"
     ]
    }
   ],
   "source": [
    "print(\"[Question]\")\n",
    "for l in gsm8k_test['question'][0].split(\".\"):\n",
    "    print(l)\n",
    "print(\"=\"*100)\n",
    "print(\"[Answer]\")\n",
    "print(gsm8k_test['answer'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabe66c",
   "metadata": {},
   "source": [
    "#### Util 함수들\n",
    "- extract_final_answer: LLM의 응답을 parse하여 최종 결과만 추출 (정답과 비교하기 위해)\n",
    "- run_benchmark_test: 벤치마크 테스트\n",
    "- save_final_result: 결과물 제출을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40c10811",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 수정해도 됩니다!\n",
    "def extract_final_answer(response: str):\n",
    "    regex = r\"(?:Answer:|Model response:)\\s*\\$?([0-9,]+)\\b|([0-9,]+)\\s*(meters|cups|miles|minutes)\"\n",
    "    matches = re.finditer(regex, response, re.MULTILINE)\n",
    "    results = [match.group(1) if match.group(1) else match.group(2).replace(\",\", \"\") for match in matches]\n",
    "\n",
    "    if len(results) == 0:\n",
    "        additional_regex = r\"\\$?([0-9,]+)\"\n",
    "        additional_matches = re.finditer(additional_regex, response, re.MULTILINE)\n",
    "        results.extend([match.group(1).replace(\",\", \"\") for match in additional_matches])\n",
    "\n",
    "    return results[-1] if results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 수정해도 됩니다!\n",
    "def run_benchmark_test(\n",
    "        dataset,\n",
    "        prompt: str,\n",
    "        model: str = \"llama3-8b-8192\",\n",
    "        num_samples: int = 50,\n",
    "        VERBOSE: bool = False\n",
    "    ):\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(min(num_samples, len(dataset)))):\n",
    "        question = dataset[i][\"question\"]\n",
    "        correct_answer = float(re.findall(r'\\d+(?:\\.\\d+)?', dataset[i][\"answer\"].split('####')[-1])[0])\n",
    "\n",
    "        response = generate_response_using_Llama(\n",
    "            prompt=prompt.format(question=question),\n",
    "            model=model\n",
    "        )\n",
    "\n",
    "        if response:\n",
    "            if VERBOSE:\n",
    "                print(\"=\"*50)\n",
    "                print(response)\n",
    "                print(\"=\"*50)\n",
    "            predicted_answer = extract_final_answer(response)\n",
    "\n",
    "            if isinstance(predicted_answer, str):\n",
    "                predicted_answer = float(predicted_answer.replace(\",\", \"\"))\n",
    "            \n",
    "            diff = abs(predicted_answer - correct_answer)\n",
    "            is_correct = diff < 1e-5 if predicted_answer is not None else False\n",
    "            \n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'correct_answer': correct_answer,\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'response': response,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "\n",
    "            if (i + 1) % 5 == 0:\n",
    "                current_acc = correct/total if total > 0 else 0\n",
    "                print(f\"Progress: [{i+1}/{num_samples}]\")\n",
    "                print(f\"Current Acc.: [{current_acc:.2%}]\")\n",
    "\n",
    "    return results, correct/total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_result(results: List[Dict[str, Any]], accuracy: float, filename: str) -> None:\n",
    "    result_str = f\"====== ACCURACY: {accuracy} ======\\n\\n\"\n",
    "    result_str += f\"[Details]\\n\"\n",
    "    \n",
    "    for idx, result in enumerate(results):\n",
    "        result_str += f\"Question {idx+1}: {result['question']}\\n\"\n",
    "        result_str += f\"Correct Answer: {result['correct_answer']}\\n\"\n",
    "        result_str += f\"Predicted Answer: {result['predicted_answer']}\\n\"\n",
    "        result_str += f\"Correct: {result['correct']}\\n\\n\"\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498f9ed",
   "metadata": {},
   "source": [
    "#### Direct prompting with few-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_direct_prompt(num_examples: int = 3) -> str:\n",
    "    train_dataset = gsm8k_train\n",
    "\n",
    "    sampled_indices = random.sample(\n",
    "        [i for i in range(len(train_dataset['question']))],\n",
    "        num_examples\n",
    "    )\n",
    "\n",
    "    prompt = \"Instruction:\\nSolve the following mathematical question and generate ONLY the answer after a tag, 'Answer:' without any rationale.\\n\"\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        cur_question = train_dataset['question'][i]\n",
    "        cur_answer = train_dataset['answer'][i].split(\"####\")[-1].strip()\n",
    "\n",
    "        prompt += f\"\\n[Example {i+1}]\\n\"\n",
    "        prompt += f\"Question:\\n{cur_question}\\n\"\n",
    "        prompt += f\"Answer:{cur_answer}\\n\"\n",
    "\n",
    "    prompt += \"\\nQuestion:\\n{question}\\nAnswer:\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2b484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:02<00:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [5/10]\n",
      "Current Acc.: [20.00%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [10/10]\n",
      "Current Acc.: [30.00%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### 어떤 방식으로 저장되는지 확인해보세요!\n",
    "PROMPT = construct_direct_prompt(3)\n",
    "VERBOSE = False\n",
    "\n",
    "results, accuracy = run_benchmark_test(\n",
    "    dataset=gsm8k_test,\n",
    "    prompt=PROMPT,\n",
    "    VERBOSE=VERBOSE,\n",
    "    num_samples=10\n",
    ")\n",
    "save_final_result(results, accuracy, \"example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41874896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 0 shot, 3 shot, 5 shot direct prompting을 통해 벤치마크 테스트를 한 후, 각각 direct_prompting_{shot: int}.txt로 저장해주세요!\n",
    "# 예시: shot이 5인 경우 direct_prompting_5.txt\n",
    "# 항상 num_samples=50 입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd21d1",
   "metadata": {},
   "source": [
    "### Chain-of-Thought prompting with few-shot example\n",
    "```text\n",
    "[Question]\n",
    "Janet’s ducks lay 16 eggs per day\n",
    " She eats three for breakfast every morning and bakes muffins for her friends every day with four\n",
    " She sells the remainder at the farmers' market daily for $2 per fresh duck egg\n",
    " How much in dollars does she make every day at the farmers' market?\n",
    "====================================================================================================\n",
    "[Answer]\n",
    "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
    "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
    "#### 18\n",
    "```\n",
    "\n",
    "[Answer] 아래의 정답을 도출하는 과정을 예시로 달아주면 CoT의 few shot이 되겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a989e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_CoT_prompt(num_examples: int = 3) -> str:\n",
    "    train_dataset = gsm8k_train\n",
    "\n",
    "    sampled_indices = random.sample(\n",
    "        [i for i in range(len(train_dataset['question']))],\n",
    "        num_examples\n",
    "    )\n",
    "    prompt = \"CoT PROMPT\" #TODO: 프롬프트를 작성해주세요!\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        #TODO: CoT example을 만들어주세요!\n",
    "        pass\n",
    "\n",
    "    prompt += \"\\nQuestion:\\n{question}\\nAnswer:\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c510586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 0 shot, 3 shot, 5 shot CoT prompting을 통해 벤치마크 테스트를 한 후, 각각 CoT_prompting_{shot: int}.txt로 저장해주세요!\n",
    "# 예시: shot이 5인 경우 CoT_prompting_5.txt\n",
    "# 항상 num_samples=50 입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c45b54",
   "metadata": {},
   "source": [
    "### Construct your prompt!!\n",
    "\n",
    "목표: 본인만의 프롬프트를 통해 정답률을 더 끌어올려보기!\n",
    "- gsm8k의 train 데이터셋에서 예시를 가져온 다음 (자유롭게!)\n",
    "- 그 예시들에 대한 풀이 과정을 만들어주세요!\n",
    "- 모든 것들이 자유입니다! Direct Prompting, CoT Prompting을 한 결과보다 정답률만 높으면 돼요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09062c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 자유롭게 수정해도 됩니다! 완전히 새로 함수를 만들어도 돼요.\n",
    "def construct_my_prompt(example_list: List[str], num_examples: int = 3):\n",
    "    # TODO: 구현해주세요!\n",
    "\n",
    "    prompt = \"Your Awesome Prompt\"\n",
    "    for example in example_list[:num_examples]:\n",
    "        prompt += f\"{example}\\n\\n\"\n",
    "    \n",
    "    prompt += \"\\nQuestion:\\n{question}\\nAnswer:\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 만든 0 shot, 3 shot, 5 shot example과 프롬프트를 통해 벤치마크 테스트를 한 후, 각각 My_prompting_{shot: int}.txt로 저장해주세요!\n",
    "# 예시: shot이 5인 경우 My_prompting_5.txt\n",
    "# 항상 num_samples=50 입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd837d7",
   "metadata": {},
   "source": [
    "### 보고서 작성하기\n",
    "#### 아래의 내용이 포함되면 됩니다!\n",
    "\n",
    "1. Direct Prompting, CoT Prompting, My Prompting을 0 shot, 3 shot, 5 shot 정답률을 표로 보여주세요!\n",
    "2. CoT Prompting이 Direct Prompting에 비해 왜 좋을 수 있는지에 대해서 서술해주세요!\n",
    "3. 본인이 작성한 프롬프트 기법이 CoT에 비해서 왜 더 좋을 수 있는지에 대해서 설명해주세요!\n",
    "4. 최종적으로, `PROMPTING.md`에 보고서를 작성해주세요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
